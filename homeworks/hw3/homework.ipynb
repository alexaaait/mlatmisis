{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Домашнее задание: LARS и варианты реализации многоклассовой классификации","metadata":{"_uuid":"37e8e36e-b7cf-4822-904b-3895b502ec5c","_cell_guid":"50b3e432-b103-4f28-9cbc-d8016c29510d","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"## Задание 1: Реализация регрессии с наименьшими углами (LARS)\nРегрессия с наименьшими углами (LARS) — это алгоритм регрессии, который выбирает признаки пошагово, что делает его подходящим для задач с большим количеством признаков. Цель — найти подмножество признаков, которые наилучшим образом объясняют целевую переменную.\n\n- LARS начинается с нуля для всех коэффициентов.\n\n- Алгоритм находит признак, наиболее коррелирующий с откликом, и обновляет коэффициент этого признака до его значения по методу наименьших квадратов, пока не будут включены все признаки или не выполнено условие остановки.\n\nТут можно прочитать подробнее про алгоритм работы https://www.geeksforgeeks.org/least-angle-regression-lars/","metadata":{"_uuid":"c024c598-6abf-4fba-acbe-ae4a45ccdb9e","_cell_guid":"807fb6cc-8bf1-4c9f-9401-5daeb90bfde7","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import r2_score\nfrom sklearn.linear_model import Lars\n\nclass LARS:\n    def __init__(self):\n        self.coefs_ = None\n        self.mean_X_ = None\n        self.mean_y_ = None\n        self.std_X_ = None\n\n    def fit(self, X, y):\n        X = X.copy().astype(np.float64)\n        y = y.copy().astype(np.float64)\n        n_samples, n_features = X.shape\n        self.coefs_ = np.zeros(n_features)\n\n        # Center data\n        self.mean_X_ = np.mean(X, axis=0)\n        self.mean_y_ = np.mean(y)\n        X_centered = X - self.mean_X_\n        y_centered = y - self.mean_y_\n\n        # Standardize features\n        self.std_X_ = np.std(X_centered, axis=0)\n        self.std_X_[self.std_X_ == 0] = 1.0\n        X_scaled = X_centered / self.std_X_\n\n        active_set = []\n        inactive_set = list(range(n_features))\n        current_pred = np.zeros(n_samples)\n        correlation_matrix = np.dot(X_scaled.T, X_scaled)\n        max_steps = min(n_features, 500)\n\n        for _ in range(max_steps):\n            residuals = y_centered - current_pred\n            correlations = np.dot(X_scaled.T, residuals)\n            max_corr = np.max(np.abs(correlations))\n\n            if max_corr < 1e-10:\n                break\n\n            next_feature = np.argmax(np.abs(correlations))\n            if next_feature not in active_set:\n                active_set.append(next_feature)\n                inactive_set.remove(next_feature)\n\n            active_indices = np.array(active_set)\n            corr_signs = np.sign(correlations[active_indices])\n\n            G_active = correlation_matrix[np.ix_(active_indices, active_indices)]\n            G_inv = np.linalg.inv(G_active + 1e-10 * np.eye(len(active_indices)))\n            direction_vector = np.dot(G_inv, corr_signs)\n            direction_norm = 1 / np.sqrt(np.sum(corr_signs * direction_vector))\n            step_direction = direction_norm * direction_vector\n            update_vector = np.dot(X_scaled[:, active_indices], step_direction)\n\n            step_sizes = []\n            for idx in inactive_set:\n                corr_diff_plus = (max_corr - correlations[idx]) / (direction_norm - update_vector[idx]) if direction_norm != update_vector[idx] else np.inf\n                corr_diff_minus = (max_corr + correlations[idx]) / (direction_norm + update_vector[idx]) if direction_norm != -update_vector[idx] else np.inf\n                step_sizes.extend([corr_diff_plus, corr_diff_minus])\n\n            gamma = min([s for s in step_sizes if s > 1e-10], default=max_corr / direction_norm)\n\n            current_pred += gamma * update_vector\n            self.coefs_[active_indices] += gamma * step_direction\n\n        return self\n\n    def predict(self, X):\n        X = X.copy().astype(np.float64)\n        X_centered = X - self.mean_X_\n        X_scaled = X_centered / self.std_X_\n        return np.dot(X_scaled, self.coefs_) + self.mean_y_\n\nX_train = np.array([[1], [2], [3], [4], [5]])\ny_train = np.array([2, 3, 5, 7, 11])\nmodel = LARS()\nmodel.fit(X_train, y_train)\npreds = model.predict(X_train)","metadata":{"_uuid":"975c9d66-c3ff-4c22-b9ed-52ef9a52e2f9","_cell_guid":"cf77a0e0-9758-4a74-80e0-033035fc2b95","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Проверки для проверки предсказаний\nassert preds.shape == (5,), \"Предсказания должны соответствовать количеству образцов\"\nassert r2_score(y_train, preds) > 0.8, \"R2 должна быть выше 0.8\"","metadata":{"_uuid":"f66e3b9b-90e2-4736-a621-5f33cbe3bbdb","_cell_guid":"884449c1-e62e-4c46-9b13-e817543ce8c0","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Сравнение с реализацией из sklearn\nsklearn_model = Lars()\nsklearn_model.fit(X_train, y_train)\nsklearn_preds = sklearn_model.predict(X_train)\n\nassert np.allclose(preds, sklearn_preds, rtol=1e-03), \"Предсказания должны совпадать с реализацией sklearn\"","metadata":{"_uuid":"cbd05951-81eb-435f-a24f-6d94c0c8eb95","_cell_guid":"83cf5e0d-bfc1-4165-aae3-8cd21b59fa52","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# plt.scatter(X_train, y_train, color='blue', label=\"Истинные значения\")\n# plt.plot(X_train, preds, color='red', label=\"Предсказанные значения (LARS\")\n# plt.plot(X_train, sklearn_preds, color='green', linestyle='dashed', label=\"Предсказанные значения (sklearn)\")\n# plt.legend()\n# plt.title(\"Регрессия LARS: Истинные vs Предсказанные значения\")\n# plt.xlabel(\"X\")\n# plt.ylabel(\"y\")\n# plt.show()","metadata":{"_uuid":"223571bd-9a4f-4d45-a326-bce1c1083768","_cell_guid":"4d8cfcce-311d-48bb-afc3-02dbd0d96d65","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Задание 2: Классификация Один-против-Всех и Все-против-Всех с логистической регрессией\nВ многоклассовой классификации два популярных подхода:\n\n- Один-против-Всех (OvA): Для каждого класса обучается отдельный классификатор, который отделяет этот класс от всех остальных.\n- Все-против-Всех (AvA): Обучается классификатор для каждой пары классов.\n\nhttps://education.yandex.ru/handbook/ml/article/linear-models#:~:text=%D0%9C%D0%BD%D0%BE%D0%B3%D0%BE%D0%BA%D0%BB%D0%B0%D1%81%D1%81%D0%BE%D0%B2%D0%B0%D1%8F%20%D0%BA%D0%BB%D0%B0%D1%81%D1%81%D0%B8%D1%84%D0%B8%D0%BA%D0%B0%D1%86%D0%B8%D1%8F-,%D0%9C%D0%BD%D0%BE%D0%B3%D0%BE%D0%BA%D0%BB%D0%B0%D1%81%D1%81%D0%BE%D0%B2%D0%B0%D1%8F%20%D0%BA%D0%BB%D0%B0%D1%81%D1%81%D0%B8%D1%84%D0%B8%D0%BA%D0%B0%D1%86%D0%B8%D1%8F,-%D0%92%20%D1%8D%D1%82%D0%BE%D0%BC%20%D1%80%D0%B0%D0%B7%D0%B4%D0%B5%D0%BB%D0%B5","metadata":{"_uuid":"0f2293e4-fd37-462c-a5c2-7aff0adfbcfe","_cell_guid":"e3830d42-d8f8-4c6e-88d9-da79c48b1e94","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"def plot_ova_classifier(classifier, X, y):\n    plt.figure(figsize=(10, 6))\n    colors = ['blue', 'green', 'red']\n    for i in np.unique(y):\n        plt.scatter(X[y == i][:, 0], X[y == i][:, 1], color=colors[i], label=f\"Класс {i}\")\n    \n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01),\n                         np.arange(y_min, y_max, 0.01))\n    \n    # Z_probabilities = classifier.predict_proba(np.c_[xx.ravel(), yy.ravel()])\n    # Z = np.max(Z_probabilities, axis=1).reshape(xx.shape)\n    \n    Z = classifier.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n    \n    plt.contourf(xx, yy, Z, alpha=0.7, cmap='coolwarm')\n    \n    # plt.scatter(X[:, 0], X[:, 1], color='black')\n    plt.title(\"One-vs-All\")\n    plt.xlabel(\"Признак 1\")\n    plt.ylabel(\"Признак 2\")\n    # plt.colorbar(label=\"Вероятность\")\n    plt.legend()\n    plt.show()\n\n\ndef plot_ava_classifier(classifier, X, y):\n    plt.figure(figsize=(10, 6))\n    colors = ['blue', 'green', 'red']\n    for i in np.unique(y):\n        plt.scatter(X[y == i][:, 0], X[y == i][:, 1], color=colors[i], label=f\"Класс {i}\")\n    \n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01),\n                         np.arange(y_min, y_max, 0.01))\n    \n    # Z_probabilities = classifier.predict_proba(np.c_[xx.ravel(), yy.ravel()])\n    # Z = np.max(Z_probabilities, axis=1).reshape(xx.shape)\n    Z = classifier.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n    \n    plt.contourf(xx, yy, Z, alpha=0.7, cmap='coolwarm')\n    \n    # plt.scatter(X[:, 0], X[:, 1], color='black')\n    plt.title(\"All-vs-All\")\n    plt.xlabel(\"Признак 1\")\n    plt.ylabel(\"Признак 2\")\n    # plt.colorbar(label=\"Вероятность\")\n    plt.legend()\n    plt.show()","metadata":{"_uuid":"fadb0b27-a2ea-4aa4-b227-95aa3f1cba48","_cell_guid":"6f0fcac2-18de-4e2b-b9e3-bdf5487d6261","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nimport seaborn as sns\n\nclass OneVsAllClassifier:\n    def __init__(self):\n        self.models = []\n        self.class_labels_ = None\n\n    def fit(self, X, y):\n        self.class_labels_ = np.unique(y)\n        self.models = []\n\n        for label in self.class_labels_:\n            binary_y = (y == label).astype(int)\n            model = LogisticRegression()\n            model.fit(X, binary_y)\n            self.models.append(model)\n\n    def predict(self, X):\n        scores = np.array([model.decision_function(X) for model in self.models]).T\n        return self.class_labels_[np.argmax(scores, axis=1)]\n\nX_test = np.array([[1, 2], [4, 5], [7, 8], [2, 3], [5, 6]])\ny_test = np.array([0, 1, 2, 0, 1])\nova_classifier = OneVsAllClassifier()\nova_classifier.fit(X_test, y_test)\nova_preds = ova_classifier.predict(X_test)\n\nassert len(ova_preds) == len(X_test), \"Предсказания должны соответствовать количеству образцов\"\nassert accuracy_score(y_test, ova_preds) > 0.8, \"Точность должна быть выше 0.8\"","metadata":{"_uuid":"06d28de2-05f2-4609-b30b-988aacf2349a","_cell_guid":"fc7ef885-5b20-4a88-a833-d7b6abecf5b2","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# plot_ova_classifier(ova_classifier, X_test, y_test)","metadata":{"_uuid":"f787c929-372c-4d8c-a426-2f321cedd364","_cell_guid":"5bd73a71-18f5-47c7-a88f-c9545c18995d","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class AllVsAllClassifier:\n    def __init__(self):\n        self.pairwise_models = {}\n        self.class_labels_ = None\n\n    def fit(self, X, y):\n        self.class_labels_ = np.unique(y)\n        self.pairwise_models = {}\n        from itertools import combinations\n        for class1, class2 in combinations(self.class_labels_, 2):\n            mask = (y == class1) | (y == class2)\n            X_pair = X[mask]\n            y_pair = (y[mask] == class2).astype(int)\n            model = LogisticRegression()\n            model.fit(X_pair, y_pair)\n            self.pairwise_models[(class1, class2)] = model\n\n    def predict(self, X):\n        votes = np.zeros((X.shape[0], len(self.class_labels_)))\n        for (class1, class2), model in self.pairwise_models.items():\n            preds = model.predict(X)\n            class1_idx = np.where(self.class_labels_ == class1)[0][0]\n            class2_idx = np.where(self.class_labels_ == class2)[0][0]\n            votes[preds == 0, class1_idx] += 1\n            votes[preds == 1, class2_idx] += 1\n        return self.class_labels_[np.argmax(votes, axis=1)]\n\nava_classifier = AllVsAllClassifier()\nava_classifier.fit(X_test, y_test)\nava_preds = ava_classifier.predict(X_test)\n\nassert len(ava_preds) == len(X_test), \"Предсказания должны соответствовать количеству образцов\"\nassert accuracy_score(y_test, ava_preds) > 0.8, \"Точность должна быть выше 0.8\"","metadata":{"_uuid":"4f537c2c-24b8-425f-9f7c-1200468d6127","_cell_guid":"490adc46-0a6a-4e68-834f-e075037922ef","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# plot_ava_classifier(ava_classifier, X_test, y_test)","metadata":{"_uuid":"9bff3aa2-e24f-4f41-b44f-7814c27db085","_cell_guid":"ea19051b-857d-41f7-8e94-17cdb8ed2ce9","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null}]}